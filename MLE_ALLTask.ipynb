{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3afd6c",
   "metadata": {},
   "source": [
    "# Sentence Transformers and Multi-Task Learning\n",
    "Objective: The goal of this exercise is to assess your ability to implement, train, and optimize neural network architectures, particularly focusing on transformers and multi-task learning extensions. Please don’t spend more than 2 hours on the exercise.\n",
    "\n",
    "Task 1: Sentence Transformer Implementation\n",
    "Implement a sentence transformer model using any deep learning framework of your choice. This model should be able to encode input sentences into fixed-length embeddings. Test your implementation with a few sample sentences and showcase the obtained embeddings. Describe any choices you had to make regarding the model architecture outside of the transformer backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f1fa3111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "599b7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformer:\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "    def encode(self, sentences):\n",
    "        inputs = self.tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "        outputs = self.model(**inputs)\n",
    "        # Use the mean of the output embeddings as the sentence embedding\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "036c7447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0792, -0.2585,  0.0439,  ..., -0.0793,  0.2279, -0.3213],\n",
      "        [ 0.1716,  0.1356,  0.3440,  ...,  0.1937,  0.0186,  0.2057],\n",
      "        [-0.1719,  0.1169,  0.1430,  ..., -0.0498,  0.2077, -0.1898],\n",
      "        [-0.0809, -0.0929, -0.4423,  ..., -0.0140,  0.1496,  0.0045],\n",
      "        [ 0.1952, -0.9000,  0.1451,  ..., -0.0153,  0.2100, -0.4170],\n",
      "        [ 0.0198, -0.7593,  0.0662,  ..., -0.3189,  0.0159, -0.2524]],\n",
      "       grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Test the Sentence Transformer Implementation\n",
    "sentences = [\"how are you?\", \"I am fine, thank you!\", \"Today i am feeling happy\", \"Today i had a bad day\", \"Did you see the new movie?\", \"Did you heard the news of accident?\"]\n",
    "model = SentenceTransformer()\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663203a0",
   "metadata": {},
   "source": [
    "In this implementation, I have used the pre-trained BERT model as the transformer backbone to generate sentence embeddings. \n",
    "\n",
    "Choices made outside the model architecture of the transformer backbone are:\n",
    "\n",
    "1. Pooling Strategy: Used the mean pooling of the output embeddings across the sequence dimension (dim=1) to obtain a single fixed-length embedding for each sentence. This is a common approach to obtain sentence representations from transformer models like BERT. Mean pooling is a simple and effective way to aggregate the sequence of output embeddings into a single vector representation while considering all token embeddings.\n",
    "\n",
    "2. Input Representation: Used the pre-trained BERT tokenizer to tokenize the input sentences and handle padding and truncation. The BERT tokenizer is specifically designed for the BERT model and handles tokenization, padding, and truncation efficiently. Using the pre-trained tokenizer ensures that the input is properly processed and compatible with the pre-trained model weights.\n",
    "\n",
    "3. Output Representation: The sentence embeddings are directly obtained from the mean-pooled output embeddings of the transformer model, without any additional transformation or projection. The output embeddings from the pre-trained BERT model are already highly informative and can be used directly for many downstream tasks without additional transformations. This simplifies the model architecture and reduces the number of trainable parameters.\n",
    "\n",
    "To test the implementation, I define a list of sample sentences, create an instance of the SentenceTransformer class, and call the encode method with the sample sentences. The resulting sentence embeddings are printed as can seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82dd42",
   "metadata": {},
   "source": [
    "# Task 2: Multi-Task Learning Expansion\n",
    "Expand the sentence transformer to handle a multi-task learning setting.\n",
    "\n",
    "Task A: Sentence Classification – Classify sentences into predefined classes (you can make these up).\n",
    "\n",
    "Task B: [Choose another relevant NLP task such as Named Entity Recognition, Sentiment Analysis, etc.] (you can make the labels up)\n",
    "\n",
    "Describe the changes made to the architecture to support multi-task learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "463aae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Importing MultiTaskSentenceTransformer...\")\n",
    "\n",
    "class MultiTaskSentenceTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', num_classes=3, num_sentiment_classes=3):\n",
    "        super(MultiTaskSentenceTransformer, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.classification_head = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        self.sentiment_head = nn.Linear(self.bert.config.hidden_size, num_sentiment_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "        classification_logits = self.classification_head(pooled_output)\n",
    "        sentiment_logits = self.sentiment_head(pooled_output)\n",
    "        return classification_logits, sentiment_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f72eb0",
   "metadata": {},
   "source": [
    "To address Task 2, the following changes were made to the architecture:\n",
    "\n",
    "1. Task-Specific Output Layers: Two separate output layers (self.classification_head and self.sentiment_head) were added to the model, one for each task. These layers take the shared sentence embedding as input and produce task-specific logits.\n",
    "2. Multi-Task Output: The forward method now returns the logits for both tasks simultaneously, allowing the model to handle a multi-task learning setting.\n",
    "3. Task-Specific Parameters: The __init__ method takes two additional parameters, num_classes and num_sentiment_classes, which determine the number of output units in the task-specific output layers. This allows the model to handle different numbers of classes for each task.\n",
    "\n",
    "By sharing the transformer backbone (BERT) and obtaining a shared sentence embedding, the model can leverage the general language understanding capabilities learned during pre-training. At the same time, the task-specific output layers allow the model to specialize for each task, enabling multi-task learning.\n",
    "To complete the assignment, you would need to provide the task descriptions and make up the labels or classes for each task. For example:\n",
    "Task A: Sentence Classification\n",
    "\n",
    "Classify sentences into predefined categories (e.g., 0 = Negative, 1 = Neutral, 2 = Positive).\n",
    "\n",
    "Task B: Sentiment Analysis\n",
    "\n",
    "Classify the sentiment expressed in a sentence (e.g., 0 = Negative, 1 = Neutral, 2 = Positive).\n",
    "\n",
    "In this example, both tasks have the same set of labels (0 = Negative, 1 = Neutral, 2 = Positive), but you can define different labels or classes based on your specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f016c02",
   "metadata": {},
   "source": [
    "# Testing the Multi-Task Learning Implementation:\n",
    "1. Created Dummy dataset contain 6 sentence with labels for both classificaiotn as well as sentiment analysis.\n",
    "2. Train the model and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1fbc243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, tokenizer, sentences, labels_classification, labels_sentiment):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = sentences\n",
    "        self.labels_classification = labels_classification\n",
    "        self.labels_sentiment = labels_sentiment\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(self.sentences[idx], return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels_classification': torch.tensor(self.labels_classification[idx], dtype=torch.long),\n",
    "            'labels_sentiment': torch.tensor(self.labels_sentiment[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def get_dataloader(tokenizer):\n",
    "    sentences = [\"how are you?\", \"I am fine, thank you!\", \"Today i am feeling happy\", \"Today i had a bad day\", \"Did you see the new movie?\", \"Did you heard the news of accident?\"]\n",
    "    labels_classification = [0, 1, 1, 2, 0, 2]  # example classification labels\n",
    "    labels_sentiment = [0, 1, 1, 2, 0, 2]  # example sentiment labels (0: neutral, 1: positive, 2: negative)\n",
    "    \n",
    "    dataset = DummyDataset(tokenizer, sentences, labels_classification, labels_sentiment)\n",
    "    return DataLoader(dataset, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "49434add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 2.29610538482666\n",
      "Epoch 2, Average Loss: 1.5332192182540894\n",
      "Epoch 3, Average Loss: 0.8149767518043518\n",
      "Epoch 4, Average Loss: 0.32415128747622174\n",
      "Epoch 5, Average Loss: 0.13112840056419373\n",
      "Classification logits:\n",
      "tensor([[ 2.0656, -1.4048, -1.5301],\n",
      "        [-1.6086,  2.6013, -1.0429],\n",
      "        [-1.5681,  2.5274, -0.8521],\n",
      "        [-0.9333, -0.6810,  1.9187],\n",
      "        [ 1.2811, -1.3929, -1.0535],\n",
      "        [-0.4313, -1.1940,  1.4762]])\n",
      "\n",
      "Sentiment logits:\n",
      "tensor([[ 2.4719, -1.1752, -1.2819],\n",
      "        [-1.3100,  2.6512, -1.1965],\n",
      "        [-1.6644,  2.6602, -0.9892],\n",
      "        [-1.0798, -0.4050,  1.9418],\n",
      "        [ 1.6587, -1.0577, -0.7372],\n",
      "        [-0.1839, -1.1173,  1.4608]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AdamW, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_multitask_model(model, train_dataloader, epochs=5):\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels_classification = batch['labels_classification']\n",
    "            labels_sentiment = batch['labels_sentiment']\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            classification_logits, sentiment_logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            loss_classification = nn.CrossEntropyLoss()(classification_logits, labels_classification)\n",
    "            loss_sentiment = nn.CrossEntropyLoss()(sentiment_logits, labels_sentiment)\n",
    "            loss = loss_classification + loss_sentiment\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        average_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}, Average Loss: {average_loss}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "model = MultiTaskSentenceTransformer(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Get DataLoader\n",
    "dataloader = get_dataloader(tokenizer)\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_multitask_model(model, dataloader)\n",
    "\n",
    "# Inference\n",
    "sentences = [\"How are you?\", \"I am fine, thank you!\", \"Today I am feeling happy\", \"Today I had a bad day\", \"Did you see the new movie?\", \"Did you hear the news of the accident?\"]\n",
    "\n",
    "# Tokenize sentences\n",
    "inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Forward pass\n",
    "trained_model.eval() \n",
    "with torch.no_grad():\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    classification_logits, sentiment_logits = trained_model(input_ids, attention_mask)\n",
    "\n",
    "print(\"Classification logits:\")\n",
    "print(classification_logits)\n",
    "print(\"\\nSentiment logits:\")\n",
    "print(sentiment_logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df8560",
   "metadata": {},
   "source": [
    "# Task 3: Training Considerations\n",
    "Discuss the implications and advantages of each scenario and explain your rationale as to how the model should be trained given the following: \n",
    "1. If the entire network should be frozen.\n",
    "- Freezing the entire network means that no part of the model, including the transformer backbone (BERT) and the task-specific output layers (self.classification_head and self.sentiment_head), will be updated during training. This scenario is generally not recommended because it prevents the model from learning and adapting to the specific tasks and datasets at hand. Freezing the entire network would only be useful if the pre-trained model was already optimized for the exact tasks and datasets you are working with, which is highly unlikely. In this case, the model would essentially act as a feature extractor, using the pre-trained representations without any fine-tuning or adaptation. This approach would likely lead to suboptimal performance, as the model would not be able to leverage the task-specific data to improve its representations and output layers.\n",
    "2. If only the transformer backbone should be frozen.\n",
    "- In this scenario, the pre-trained transformer backbone (BERT) is frozen, while the task-specific output layers (self.classification_head and self.sentiment_head) are allowed to be updated during training. Freezing the transformer backbone can be a reasonable approach, especially when working with limited task-specific data. By freezing the backbone, you can leverage the knowledge and representations learned from large-scale pre-training on general language data, which can be beneficial for various NLP tasks. Allowing the task-specific output layers to be updated enables the model to adapt to the specific tasks and datasets, while still benefiting from the pre-trained representations in the backbone. This approach can be particularly useful when the tasks are closely related to the pre-training objective (e.g., masked language modeling) and when there is a significant domain shift between the pre-training data and the task-specific data. However, it may limit the model's ability to adapt the backbone representations to the specific tasks, potentially hindering performance in some cases.\n",
    "\n",
    "3. If only one of the task-specific heads (either for Task A or Task B) should be frozen.\n",
    "\n",
    "- In this scenario, one of the task-specific output layers (either self.classification_head or self.sentiment_head) is frozen, while the transformer backbone and the other task-specific head are allowed to be updated during training. Freezing one of the task-specific heads can be useful in a transfer learning scenario, where you have a well-performing model for one task (e.g., Task A) and want to leverage its knowledge for the other task (e.g., Task B). By freezing the Task A head (self.classification_head), you can preserve the knowledge and representations learned for that task, while allowing the transformer backbone and the Task B head (self.sentiment_head) to be updated and adapted to the new task. This approach can be beneficial when the tasks are related, as the shared transformer backbone can leverage the knowledge from the frozen task-specific head to improve the representations for the new task. However, it may limit the model's ability to fully adapt to the new task, as the frozen task-specific head cannot be updated based on the new task data.\n",
    "\n",
    "Consider a scenario where transfer learning can be beneficial. Explain how you would approach the transfer learning process, including:\n",
    "1. The choice of a pre-trained model.\n",
    "- Choose a pre-trained language model that has been trained on a large and diverse corpus of text data, such as BERT, or GPT-2. The choice would depend on the task at hand, the domain of the data, and the architectural preferences.\n",
    "2. The layers you would freeze/unfreeze.\n",
    "- For most transfer learning scenarios, you would freeze the majority of the pre-trained model's layers, especially the lower layers that capture general language representation. Unfreeze and fine-tune the top few layers, allowing them to adapt to the specific task and domain. If the task is significantly different from the pre-training objective (e.g., sequence generation vs. classification), you might consider unfreezing more layers to allow for more adaptation.\n",
    "3. The rationale behind these choices.\n",
    "- The lower layers of pre-trained language models capture general language representations and patterns, which are often transferable across tasks and domains. By freezing these lower layers, you can leverage the knowledge acquired during pre-training while allowing the top layers to adapt to the specific task and domain. Unfreezing and fine-tuning the top layers enables the model to specialize for the target task and learn task-specific representations, while still benefiting from the general language knowledge in the lower layers. This approach helps to prevent catastrophic forgetting of the general language knowledge while enabling effective transfer learning and domain adaptation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadfaab3",
   "metadata": {},
   "source": [
    "# Task 4: Layer-wise Learning Rate Implementation (BONUS)  -May not be in perfect shape\n",
    "\n",
    "1. Implement layer-wise learning rates for the multi-task sentence transformer.\n",
    "- Define Learning Rates: Decide on the learning rates for different layers. You can use a dictionary or a list to store these values.\n",
    "- Group Model Parameters: Group the model parameters by layer and assign the corresponding learning rates.\n",
    "- Update Optimizer: Use a learning rate scheduler that handles layer-wise learning rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8578413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get optimizer with layer-wise learning rates may not be in perfect shape.\n",
    "def get_optimizer(model, base_lr=2e-5):\n",
    "    lr_dict = {\n",
    "        'bert.encoder.layer.0': base_lr * 0.1,\n",
    "        'bert.encoder.layer.1': base_lr * 0.1,\n",
    "        'bert.encoder.layer.2': base_lr * 0.1,\n",
    "        'bert.encoder.layer.3': base_lr * 0.1,\n",
    "        'bert.encoder.layer.4': base_lr * 0.2,\n",
    "        'bert.encoder.layer.5': base_lr * 0.2,\n",
    "        'bert.encoder.layer.6': base_lr * 0.2,\n",
    "        'bert.encoder.layer.7': base_lr * 0.2,\n",
    "        'bert.encoder.layer.8': base_lr * 0.5,\n",
    "        'bert.encoder.layer.9': base_lr * 0.5,\n",
    "        'bert.encoder.layer.10': base_lr * 0.5,\n",
    "        'bert.encoder.layer.11': base_lr * 0.5,\n",
    "        'classification_head': base_lr,\n",
    "        'sentiment_head': base_lr\n",
    "    }\n",
    "\n",
    "    optimizer_grouped_parameters = []\n",
    "    for name, param in model.named_parameters():\n",
    "        for layer, lr in lr_dict.items():\n",
    "            if layer in name:\n",
    "                optimizer_grouped_parameters.append({'params': [param], 'lr': lr})\n",
    "                break\n",
    "        else:\n",
    "            optimizer_grouped_parameters.append({'params': [param], 'lr': base_lr})\n",
    "    \n",
    "    return AdamW(optimizer_grouped_parameters, lr=base_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673321aa",
   "metadata": {},
   "source": [
    "1. Explain the rationale for the specific learning rates you've set for each layer.\n",
    "- Lower layers of a pre-trained model capture more general features of the input data, and thus, they may need smaller learning rates to fine-tune these features without forgetting them.\n",
    "- Higher layers of the model capture more task-specific features, and they may benefit from larger learning rates to adapt to the new task more quickly.\n",
    "- In a multi-task setting, different tasks may require different levels of fine-tuning. For example, sentiment analysis may require more adjustments in the higher layers, while classification tasks may benefit from more balanced adjustments across layers.\n",
    "\n",
    "2. Describe the potential benefits of using layer-wise learning rates for training deep neural networks. Does the multi-task setting play into that?\n",
    "\n",
    "A) Potential Benefits\n",
    "\n",
    "- Different layers in a neural network can learn at different rates. Early layers may require a smaller learning rate to fine-tune basic feature detectors, while later layers might benefit from a larger learning rate to quickly learn more complex patterns.\n",
    "- Properly adjusting learning rates for different layers can help mitigate issues of vanishing or exploding gradients, particularly in very deep networks.\n",
    "- Smaller learning rates for lower layers can help preserve learned features that are generally useful, while higher layers can adapt to specific tasks or datasets with larger learning rates.\n",
    "- By controlling the learning rates of different layers, it can serve as a form of regularization, reducing overfitting and improving the model’s ability to generalize to new data.\n",
    "\n",
    "B) Multi-task Learning Setting\n",
    "In a multi-task learning setting, the benefits of layer-wise learning rates can be even more pronounced due to the following reasons:\n",
    "\n",
    "- Multi-task learning often involves shared lower layers and task-specific higher layers. Layer-wise learning rates allow for more nuanced control, enabling shared layers to learn general representations effectively, while task-specific layers adapt quickly to their respective tasks.\n",
    "- Different tasks might have different learning dynamics. Layer-wise learning rates can help balance the training, ensuring that no single task disproportionately influences the shared layers, leading to better overall performance.\n",
    "\n",
    "- In multi-task learning, gradients from different tasks can interfere with each other. By adjusting learning rates layer-wise, one can mitigate such interference, ensuring stable and effective training for all tasks.\n",
    "Gradient Magnitude Differences: Tasks with varying gradient magnitudes can be accommodated better by using appropriate learning rates for different layers, avoiding situations where gradients from some tasks might dominate or be overshadowed by others.\n",
    "Enhanced Learning Efficiency:\n",
    "\n",
    "In summary, layer-wise learning rates can significantly enhance the training of deep neural networks, providing faster convergence, better generalization, and improved stability. In multi-task learning, they play a crucial role in balancing the learning dynamics across different tasks, ensuring effective and efficient training of shared and task-specific layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453772cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
